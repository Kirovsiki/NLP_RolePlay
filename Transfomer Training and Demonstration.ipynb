{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8825aea",
   "metadata": {},
   "source": [
    "### Role Play: Personality Fixation NLP Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424b721e",
   "metadata": {},
   "source": [
    "##### First, import parlai's training module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a231e150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parlai.scripts.train_model import TrainModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21277a1b",
   "metadata": {},
   "source": [
    "##### Next, we will import the bootstrap files, which are used to store the preprocessing and bootstrapping of the dataset to play the role of the teacher. so that we can effectively train and evaluate our model with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8b9984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from main.tasks.empathetic_dialogues.agents import *\n",
    "#from main.tasks.wizard_of_wikipedia.agents import *\n",
    "#from main.tasks.guguaitrain.agents import *  \n",
    "#from main.tasks.blended_skill_talk.agents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b6d9f",
   "metadata": {},
   "source": [
    "##### These are some basic parameters:\n",
    "'task': specifies the task or dataset for training. Here you list four tasks: empathetic_dialogues, wizard_of_wikipedia, guguaitrain, and blended_skill_talk.\n",
    "\n",
    "'model': type of model. Here transformer/generator is used, referring to a generative model based on Transformer.\n",
    "\n",
    "'model_file': The path where the model is saved.\n",
    "\n",
    "'init_model': Path to initialize the model. This is typically used to start training from a pre-trained model.\n",
    "\n",
    "'dict_file': Path to the file for the dictionary, which contains the vocabulary used for training.\n",
    "\n",
    "'fp16': whether to use 16-bit floating point numbers for training, which speeds up training and reduces memory usage.\n",
    "\n",
    "'embedding_size': the dimension of the word embedding.\n",
    "\n",
    "'ffn_size': the size of the feed-forward network in the Transformer.\n",
    "\n",
    "'n_positions': Maximum sequence length that the model can handle.\n",
    "\n",
    "'dropout' and 'attention_dropout': represent the regular dropout and the dropout in the attention mechanism, respectively.\n",
    "\n",
    "'n_layers': the number of layers in the Transformer model.\n",
    "\n",
    "'n_heads': the number of attention heads in the Transformer model.\n",
    "\n",
    "'learn_positional_embeddings': whether to learn positional embeddings.\n",
    "\n",
    "'variant': the variant of Transformer. Here xlm, a multilingual Transformer model, is used.\n",
    "\n",
    "'num_epochs': The number of training cycles.\n",
    "\n",
    "'validation_every_n_secs': how many seconds to perform validation.\n",
    "\n",
    "'batchsize': number of samples per batch.\n",
    "\n",
    "'activation': activation function, GELU is used here.\n",
    "\n",
    "'optimizer': optimizer, here Adamax is used.\n",
    "\n",
    "'lr_scheduler': learning rate scheduling strategy, here is a fixed learning rate.\n",
    "\n",
    "'gradient_clip': threshold for gradient clipping.\n",
    "\n",
    "'dict_tokenizer': the way to split text, here BPE (Byte Pair Encoding) is used.\n",
    "\n",
    "'dict_lower': whether to convert text to lowercase.\n",
    "\n",
    "'lr': learning rate.\n",
    "\n",
    "'text_truncate' and 'label_truncate': the maximum length of the input text and label respectively.\n",
    "\n",
    "'save_after_valid': whether to save the model after each validation.\n",
    "\n",
    "'gpu': the number of the GPU used.\n",
    "\n",
    "'dict_maxtokens': Maximum number of words in the dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c6bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # set up the parameters\n",
    "    params = {\n",
    "        'task': 'empathetic_dialogues,wizard_of_wikipedia,guguaitrain,blended_skill_talk',\n",
    "        'model': 'transformer/generator',\n",
    "        'model_file': 'model/guguAI20',\n",
    "        'init_model': 'data/models/blender/blender_90M/model',\n",
    "        'dict_file': 'data/models/blender/blender_90M/model.dict',\n",
    "        'fp16': True,\n",
    "        'embedding_size': 512,\n",
    "        'ffn_size': 2048,\n",
    "        'n_positions': 512,\n",
    "        'dropout': 0.1,\n",
    "        'attention_dropout': 0.0,\n",
    "        'n_layers': 8,\n",
    "        'n_heads': 16,\n",
    "        'learn_positional_embeddings': True,\n",
    "        'variant': 'xlm',\n",
    "        'num_epochs': 83,\n",
    "        'validation_every_n_secs': 3600,\n",
    "        'batchsize': 50,\n",
    "        'activation': 'gelu',\n",
    "        'optimizer': 'adamax',\n",
    "        'lr_scheduler': 'fixed',\n",
    "        'gradient_clip': 0.1,\n",
    "        'dict_tokenizer': 'bpe',\n",
    "        'dict_lower': True,\n",
    "        'lr': 1e-04,\n",
    "        'text_truncate': 512,\n",
    "        'label_truncate': 128,\n",
    "        'save_after_valid': True,\n",
    "        'gpu': 0,  # add this line to use GPU\n",
    "        'dict_maxtokens': 100000  \n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931bcedd",
   "metadata": {},
   "source": [
    "##### Define the main program as params so that it can be called during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "993c1977",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a465028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:22:43 | building dictionary first...\n",
      "22:22:43 | \u001b[33mOverriding opt[\"init_model\"] to data/models/blender/blender_90M/model (previously: model/guguAI20.checkpoint)\u001b[0m\n",
      "22:22:43 | \u001b[33mOverriding opt[\"dict_file\"] to data/models/blender/blender_90M/model.dict (previously: model/guguAI20.checkpoint.dict)\u001b[0m\n",
      "22:22:43 | \u001b[33mOverriding opt[\"num_epochs\"] to 83.0 (previously: 3.0)\u001b[0m\n",
      "22:22:43 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: download_path: None,verbose: False,datapath: C:\\Users\\97919\\Desktop\\NLP_roleplay\\data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
      "22:22:43 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
      "--num-epochs 80.0\u001b[0m\n",
      "22:22:43 | Using CUDA\n",
      "22:22:43 | loading dictionary from model/guguAI20.checkpoint.dict\n",
      "22:22:43 | num words = 54944\n",
      "22:22:43 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
      "22:22:44 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "22:22:44 | Loading existing model params from model/guguAI20.checkpoint\n",
      "22:22:45 | Opt:\n",
      "22:22:45 |     activation: gelu\n",
      "22:22:45 |     adafactor_eps: '[1e-30, 0.001]'\n",
      "22:22:45 |     adam_eps: 1e-08\n",
      "22:22:45 |     add_missing_turns: none\n",
      "22:22:45 |     add_p1_after_newln: False\n",
      "22:22:45 |     aggregate_micro: False\n",
      "22:22:45 |     allow_missing_init_opts: False\n",
      "22:22:45 |     attention_dropout: 0.0\n",
      "22:22:45 |     batchsize: 50\n",
      "22:22:45 |     beam_block_full_context: True\n",
      "22:22:45 |     beam_block_list_filename: None\n",
      "22:22:45 |     beam_block_ngram: -1\n",
      "22:22:45 |     beam_context_block_ngram: -1\n",
      "22:22:45 |     beam_delay: 30\n",
      "22:22:45 |     beam_length_penalty: 0.65\n",
      "22:22:45 |     beam_min_length: 1\n",
      "22:22:45 |     beam_size: 1\n",
      "22:22:45 |     betas: '[0.9, 0.999]'\n",
      "22:22:45 |     bpe_add_prefix_space: None\n",
      "22:22:45 |     bpe_debug: False\n",
      "22:22:45 |     bpe_dropout: None\n",
      "22:22:45 |     bpe_merge: None\n",
      "22:22:45 |     bpe_vocab: None\n",
      "22:22:45 |     checkpoint_activations: False\n",
      "22:22:45 |     chosen_topic_delimiter: '\\n'\n",
      "22:22:45 |     clearml_log: False\n",
      "22:22:45 |     clearml_project_name: ParlAI\n",
      "22:22:45 |     clearml_task_name: 'Default Task'\n",
      "22:22:45 |     compute_tokenized_bleu: False\n",
      "22:22:45 |     datapath: C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\n",
      "22:22:45 |     datatype: train\n",
      "22:22:45 |     delimiter: '\\n'\n",
      "22:22:45 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "22:22:45 |     dict_endtoken: __end__\n",
      "22:22:45 |     dict_file: model/guguAI20.checkpoint.dict\n",
      "22:22:45 |     dict_include_test: False\n",
      "22:22:45 |     dict_include_valid: False\n",
      "22:22:45 |     dict_initpath: None\n",
      "22:22:45 |     dict_language: english\n",
      "22:22:45 |     dict_loaded: True\n",
      "22:22:45 |     dict_lower: True\n",
      "22:22:45 |     dict_max_ngram_size: -1\n",
      "22:22:45 |     dict_maxexs: -1\n",
      "22:22:45 |     dict_maxtokens: 100000\n",
      "22:22:45 |     dict_minfreq: 0\n",
      "22:22:45 |     dict_nulltoken: __null__\n",
      "22:22:45 |     dict_starttoken: __start__\n",
      "22:22:45 |     dict_textfields: text,labels\n",
      "22:22:45 |     dict_tokenizer: bpe\n",
      "22:22:45 |     dict_unktoken: __unk__\n",
      "22:22:45 |     display_examples: False\n",
      "22:22:45 |     download_path: None\n",
      "22:22:45 |     dropout: 0.1\n",
      "22:22:45 |     dynamic_batching: None\n",
      "22:22:45 |     embedding_projection: random\n",
      "22:22:45 |     embedding_size: 512\n",
      "22:22:45 |     embedding_type: random\n",
      "22:22:45 |     embeddings_scale: True\n",
      "22:22:45 |     eval_batchsize: None\n",
      "22:22:45 |     eval_dynamic_batching: None\n",
      "22:22:45 |     evaltask: None\n",
      "22:22:45 |     ffn_size: 2048\n",
      "22:22:45 |     final_extra_opt: \n",
      "22:22:45 |     force_fp16_tokens: True\n",
      "22:22:45 |     fp16: True\n",
      "22:22:45 |     fp16_impl: safe\n",
      "22:22:45 |     gpu: 0\n",
      "22:22:45 |     gpu_beam_blocking: False\n",
      "22:22:45 |     gradient_clip: 0.1\n",
      "22:22:45 |     hide_labels: False\n",
      "22:22:45 |     history_add_global_end_token: None\n",
      "22:22:45 |     history_reversed: False\n",
      "22:22:45 |     history_size: -1\n",
      "22:22:45 |     image_cropsize: 224\n",
      "22:22:45 |     image_mode: raw\n",
      "22:22:45 |     image_size: 256\n",
      "22:22:45 |     include_checked_sentence: True\n",
      "22:22:45 |     include_knowledge: True\n",
      "22:22:45 |     include_knowledge_separator: False\n",
      "22:22:45 |     inference: greedy\n",
      "22:22:45 |     init_model: model/guguAI20.checkpoint\n",
      "22:22:45 |     init_opt: None\n",
      "22:22:45 |     interactive_mode: False\n",
      "22:22:45 |     invsqrt_lr_decay_gamma: -1\n",
      "22:22:45 |     is_debug: False\n",
      "22:22:45 |     label_truncate: 128\n",
      "22:22:45 |     label_type: response\n",
      "22:22:45 |     lambda_decay: 0.9\n",
      "22:22:45 |     learn_positional_embeddings: True\n",
      "22:22:45 |     learningrate: 0.0001\n",
      "22:22:45 |     load_from_checkpoint: True\n",
      "22:22:45 |     log_every_n_secs: -1\n",
      "22:22:45 |     log_every_n_steps: 50\n",
      "22:22:45 |     log_keep_fields: all\n",
      "22:22:45 |     loglevel: info\n",
      "22:22:45 |     lr_scheduler: fixed\n",
      "22:22:45 |     lr_scheduler_decay: 0.5\n",
      "22:22:45 |     lr_scheduler_patience: 3\n",
      "22:22:45 |     max_train_steps: -1\n",
      "22:22:45 |     max_train_time: -1\n",
      "22:22:45 |     metrics: default\n",
      "22:22:45 |     model: transformer/generator\n",
      "22:22:45 |     model_file: model/guguAI20\n",
      "22:22:45 |     model_parallel: False\n",
      "22:22:45 |     momentum: 0\n",
      "22:22:45 |     multitask_weights: [1]\n",
      "22:22:45 |     mutators: None\n",
      "22:22:45 |     n_decoder_layers: -1\n",
      "22:22:45 |     n_encoder_layers: -1\n",
      "22:22:45 |     n_heads: 16\n",
      "22:22:45 |     n_layers: 8\n",
      "22:22:45 |     n_positions: 512\n",
      "22:22:45 |     n_segments: 0\n",
      "22:22:45 |     nesterov: True\n",
      "22:22:45 |     no_cuda: False\n",
      "22:22:45 |     num_epochs: 83.0\n",
      "22:22:45 |     num_topics: 5\n",
      "22:22:45 |     num_workers: 0\n",
      "22:22:45 |     nus: [0.7]\n",
      "22:22:45 |     omega_bound: 0.3\n",
      "22:22:45 |     optimizer: adamax\n",
      "22:22:45 |     output_scaling: 1.0\n",
      "22:22:45 |     override: \"{'task': 'empathetic_dialogues,wizard_of_wikipedia,guguaitrain,blended_skill_talk', 'model': 'transformer/generator', 'model_file': 'model/guguAI20', 'init_model': 'data/models/blender/blender_90M/model', 'dict_file': 'data/models/blender/blender_90M/model.dict', 'fp16': True, 'embedding_size': 512, 'ffn_size': 2048, 'n_positions': 512, 'dropout': 0.1, 'attention_dropout': 0.0, 'n_layers': 8, 'n_heads': 16, 'learn_positional_embeddings': True, 'variant': 'xlm', 'num_epochs': 83.0, 'validation_every_n_secs': 3600.0, 'batchsize': 50, 'activation': 'gelu', 'optimizer': 'adamax', 'lr_scheduler': 'fixed', 'gradient_clip': 0.1, 'dict_tokenizer': 'bpe', 'dict_lower': True, 'learningrate': 0.0001, 'text_truncate': 512, 'label_truncate': 128, 'save_after_valid': True, 'gpu': 0, 'dict_maxtokens': 100000}\"\n",
      "22:22:45 |     p_reset: True\n",
      "22:22:45 |     parlai_home: C:\\Users\\97919\\Desktop\\GUGUAI\\ParlAI-main\n",
      "22:22:45 |     person_tokens: False\n",
      "22:22:45 |     rank_candidates: False\n",
      "22:22:45 |     relu_dropout: 0.0\n",
      "22:22:45 |     save_after_valid: True\n",
      "22:22:45 |     save_every_n_secs: -1\n",
      "22:22:45 |     save_format: conversations\n",
      "22:22:45 |     seed: None\n",
      "22:22:45 |     share_word_embeddings: True\n",
      "22:22:45 |     short_final_eval: False\n",
      "22:22:45 |     skip_generation: False\n",
      "22:22:45 |     special_tok_lst: None\n",
      "22:22:45 |     split_lines: False\n",
      "22:22:45 |     starttime: Aug10_22-41\n",
      "22:22:45 |     task: empathetic_dialogues,wizard_of_wikipedia,guguaitrain,blended_skill_talk\n",
      "22:22:45 |     teacher_seed: None\n",
      "22:22:45 |     temperature: 1.0\n",
      "22:22:45 |     tensorboard_log: False\n",
      "22:22:45 |     tensorboard_logdir: None\n",
      "22:22:45 |     text_truncate: 512\n",
      "22:22:45 |     topk: 10\n",
      "22:22:45 |     topp: 0.9\n",
      "22:22:45 |     train_experiencer_only: False\n",
      "22:22:45 |     truncate: -1\n",
      "22:22:45 |     update_freq: 1\n",
      "22:22:45 |     use_reply: label\n",
      "22:22:45 |     validation_cutoff: 1.0\n",
      "22:22:45 |     validation_every_n_epochs: -1\n",
      "22:22:45 |     validation_every_n_secs: 3600.0\n",
      "22:22:45 |     validation_every_n_steps: -1\n",
      "22:22:45 |     validation_max_exs: -1\n",
      "22:22:45 |     validation_metric: accuracy\n",
      "22:22:45 |     validation_metric_mode: None\n",
      "22:22:45 |     validation_patience: 10\n",
      "22:22:45 |     validation_share_agent: False\n",
      "22:22:45 |     variant: xlm\n",
      "22:22:45 |     verbose: False\n",
      "22:22:45 |     verbose_topk: -1\n",
      "22:22:45 |     wandb_entity: None\n",
      "22:22:45 |     wandb_log: False\n",
      "22:22:45 |     wandb_log_model: False\n",
      "22:22:45 |     wandb_name: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:22:45 |     wandb_project: None\n",
      "22:22:45 |     warmup_rate: 0.0001\n",
      "22:22:45 |     warmup_updates: -1\n",
      "22:22:45 |     weight_decay: None\n",
      "22:22:45 |     world_logs: \n",
      "22:22:45 | creating task(s): empathetic_dialogues,wizard_of_wikipedia,guguaitrain,blended_skill_talk\n",
      "22:23:02 | loading C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\wizard_of_wikipedia\\train.json\n",
      "22:23:12 | \u001b[33mSome data not being used. If you are not trying to reproduce the previous results, it is recommended that you run with the flag --add-missing-turns train or --add-missing-turns all.\u001b[0m\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "22:23:16 | Loading ParlAI text data: C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\blended_skill_talk\\train.txt\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "[loading conversation data:C:\\Users\\97919\\Desktop\\NLP_roleplay\\data\\guguaitrain\\train.json]\n",
      "22:23:21 | training...\n",
      "22:23:21 | \u001b[33mparlai.tasks.guguaitrain.agents.DefaultTeacher' is outputting dicts instead of messages. If this is a teacher that is part of ParlAI, please file an issue on GitHub. If it is your own teacher, please return a Message object instead.\u001b[0m\n",
      "22:23:34 | time:58783s total_exs:11649400 total_steps:232988 epochs:69.42 time_left:11504s\n",
      "                         clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen   loss  \\\n",
      "   all                  70.59     1  5156 20695       0          0 200.7 2500             16384  4.795    .5885 20.64  .8379   \n",
      "   blended_skill_talk   160.6                         0          0       1154                                   17.99 .06091   \n",
      "   empathetic_dialogues 33.38                         0          0        375                                   17.32  2.172   \n",
      "   guguaitrain          12.08                         0          0        220                                    23.4 .02343   \n",
      "   wizard_of_wikipedia  76.32                         0          0        751                                   23.85  1.095   \n",
      "                               lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  \\\n",
      "   all                  3.125e-06  1006  4040       0          0 3.463      .7989     .4329               232988 6162 24736   \n",
      "   blended_skill_talk                               0          0 1.063      .9885     .8154                                   \n",
      "   empathetic_dialogues                             0          0 8.777      .4935     .0080                                   \n",
      "   guguaitrain                                      0          0 1.024      .9953     .9000                                   \n",
      "   wizard_of_wikipedia                              0          0 2.989      .7181   .007989                                   \n",
      "                          ups  \n",
      "   all                  4.015  \n",
      "   blended_skill_talk          \n",
      "   empathetic_dialogues        \n",
      "   guguaitrain                 \n",
      "   wizard_of_wikipedia\n",
      "\n",
      "22:23:49 | time:58798s total_exs:11651900 total_steps:233038 epochs:69.43 time_left:11492s\n",
      "                         clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen   loss  \\\n",
      "   all                  71.29     1  5116 17128       0          0 167.4 2500             16384  4.951    .6367 20.69  .8420   \n",
      "   blended_skill_talk   159.4                         0          0       1106                                   17.66 .06185   \n",
      "   empathetic_dialogues 32.75                         0          0        383                                   17.43  2.191   \n",
      "   guguaitrain          12.73                         0          0        210                                   22.86 .01977   \n",
      "   wizard_of_wikipedia  80.33                         0          0        801                                    24.8  1.095   \n",
      "                               lr  ltpb  ltps  ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  \\\n",
      "   all                  3.125e-06  1017  3406       0          0 3.505      .7991     .4451               233038 6133 20534   \n",
      "   blended_skill_talk                               0          0 1.064      .9887     .8273                                   \n",
      "   empathetic_dialogues                             0          0 8.948      .4937   .002611                                   \n",
      "   guguaitrain                                      0          0  1.02      .9975     .9429                                   \n",
      "   wizard_of_wikipedia                              0          0 2.989      .7165   .007491                                   \n",
      "                          ups  \n",
      "   all                  3.348  \n",
      "   blended_skill_talk          \n",
      "   empathetic_dialogues        \n",
      "   guguaitrain                 \n",
      "   wizard_of_wikipedia\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m TrainModel\u001b[38;5;241m.\u001b[39mmain(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32m~\\Desktop\\NLP_roleplay\\parlai\\core\\script.py:127\u001b[0m, in \u001b[0;36mParlaiScript.main\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_args(args)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m kwargs:\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_args(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Desktop\\NLP_roleplay\\parlai\\core\\script.py:92\u001b[0m, in \u001b[0;36mParlaiScript._run_kwargs\u001b[1;34m(cls, kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_args()\n\u001b[0;32m     91\u001b[0m opt \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_kwargs(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_from_parser_and_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\NLP_roleplay\\parlai\\core\\script.py:108\u001b[0m, in \u001b[0;36mParlaiScript._run_from_parser_and_opt\u001b[1;34m(cls, opt, parser)\u001b[0m\n\u001b[0;32m    106\u001b[0m script \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(opt)\n\u001b[0;32m    107\u001b[0m script\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m parser\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscript\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\NLP_roleplay\\parlai\\scripts\\train_model.py:1085\u001b[0m, in \u001b[0;36mTrainModel.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1084\u001b[0m     set_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 1085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\NLP_roleplay\\parlai\\scripts\\train_model.py:1028\u001b[0m, in \u001b[0;36mTrainLoop.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;124;03mPerform a training run.\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \n\u001b[0;32m   1025\u001b[0m \u001b[38;5;124;03m:return: tuple of reports (validation_report, test_report)\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m opt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\n\u001b[1;32m-> 1028\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _train_log \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_steps():\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;66;03m# we've already done what we need in these\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;66;03m# perform final validation/testing\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\NLP_roleplay\\parlai\\scripts\\train_model.py:935\u001b[0m, in \u001b[0;36mTrainLoop.train_steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;66;03m# do one example / batch of examples\u001b[39;00m\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 935\u001b[0m         \u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparley\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m StopTrainException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    937\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopping from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\NLP_roleplay\\parlai\\core\\worlds.py:880\u001b[0m, in \u001b[0;36mBatchWorld.parley\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    876\u001b[0m         w\u001b[38;5;241m.\u001b[39mparley_init()\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_agents):\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;66;03m# The agent acts.\u001b[39;00m\n\u001b[1;32m--> 880\u001b[0m     batch_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_act\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_observations\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macts[agent_idx] \u001b[38;5;241m=\u001b[39m batch_act\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;66;03m# We possibly execute this action in the world.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\NLP_roleplay\\parlai\\core\\worlds.py:848\u001b[0m, in \u001b[0;36mBatchWorld.batch_act\u001b[1;34m(self, agent_idx, batch_observation)\u001b[0m\n\u001b[0;32m    846\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mget_agents()[agent_idx]\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_act\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 848\u001b[0m     batch_actions \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_act\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_observation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;66;03m# Store the actions locally in each world.\u001b[39;00m\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworlds):\n",
      "File \u001b[1;32m~\\Desktop\\NLP_roleplay\\parlai\\core\\torch_agent.py:2248\u001b[0m, in \u001b[0;36mTorchAgent.batch_act\u001b[1;34m(self, observations)\u001b[0m\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_training:\n\u001b[0;32m   2246\u001b[0m     \u001b[38;5;66;03m# register the start of updates for later counting when they occur\u001b[39;00m\n\u001b[0;32m   2247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_metrics\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mups\u001b[39m\u001b[38;5;124m'\u001b[39m, GlobalTimerMetric(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m-> 2248\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2250\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m   2251\u001b[0m         \u001b[38;5;66;03m# save memory and compute by disabling autograd.\u001b[39;00m\n\u001b[0;32m   2252\u001b[0m         \u001b[38;5;66;03m# use `with torch.enable_grad()` to gain back gradients.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\NLP_roleplay\\parlai\\core\\torch_generator_agent.py:789\u001b[0m, in \u001b[0;36mTorchGeneratorAgent.train_step\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 789\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_params()\n",
      "File \u001b[1;32m~\\Desktop\\NLP_roleplay\\parlai\\core\\torch_generator_agent.py:743\u001b[0m, in \u001b[0;36mTorchGeneratorAgent.compute_loss\u001b[1;34m(self, batch, return_output)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mlabel_vec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot compute loss without a label.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 743\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_vec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    744\u001b[0m scores, preds, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m model_output\n\u001b[0;32m    745\u001b[0m score_view \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, scores\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\.conda\\envs\\new_parlai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\NLP_roleplay\\parlai\\core\\torch_generator_agent.py:316\u001b[0m, in \u001b[0;36mTorchGeneratorModel.forward\u001b[1;34m(self, ys, prev_enc, maxlen, bsz, *xs)\u001b[0m\n\u001b[0;32m    313\u001b[0m encoder_states \u001b[38;5;241m=\u001b[39m prev_enc \u001b[38;5;28;01mif\u001b[39;00m prev_enc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\u001b[38;5;241m*\u001b[39mxs)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;66;03m# use teacher forcing\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m scores, preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_forced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores, preds, encoder_states\n",
      "File \u001b[1;32m~\\Desktop\\NLP_roleplay\\parlai\\core\\torch_generator_agent.py:185\u001b[0m, in \u001b[0;36mTorchGeneratorModel.decode_forced\u001b[1;34m(self, encoder_states, ys)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Beginning of Sentence token is automatically added to the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel in decode_forced, but you included it in the label. This means \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour model will have a double BOS token, which is probably not what \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou intended.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    184\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_forced_decoder_input(bsz, inputs)\n\u001b[1;32m--> 185\u001b[0m latent, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(latent)\n\u001b[0;32m    187\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\new_parlai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\NLP_roleplay\\parlai\\agents\\transformer\\modules\\decoder.py:665\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, input, encoder_state, incr_state, **kwargs)\u001b[0m\n\u001b[0;32m    661\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_embedding(\u001b[38;5;28minput\u001b[39m, positions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    663\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(tensor)  \u001b[38;5;66;03m# --dropout\u001b[39;00m\n\u001b[1;32m--> 665\u001b[0m tensor, new_incr_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_layers(\n\u001b[0;32m    666\u001b[0m     tensor, encoder_output, encoder_mask, incr_state\u001b[38;5;241m=\u001b[39mincr_state, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    667\u001b[0m )\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariant \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprelayernorm\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    670\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_embeddings(tensor)\n",
      "File \u001b[1;32m~\\Desktop\\NLP_roleplay\\parlai\\agents\\transformer\\modules\\decoder.py:239\u001b[0m, in \u001b[0;36mBaseTransformerDecoder.forward_layers\u001b[1;34m(self, tensor, incr_state, *extra_args, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m--> 239\u001b[0m         tensor, new_incr_state[idx] \u001b[38;5;241m=\u001b[39m layer(\n\u001b[0;32m    240\u001b[0m             tensor, \u001b[38;5;241m*\u001b[39mextra_args, incr_state\u001b[38;5;241m=\u001b[39mincr_state\u001b[38;5;241m.\u001b[39mget(idx), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    241\u001b[0m         )\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor, new_incr_state\n",
      "File \u001b[1;32m~\\.conda\\envs\\new_parlai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\NLP_roleplay\\parlai\\agents\\transformer\\modules\\decoder.py:541\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, x, encoder_output, encoder_mask, incr_state, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)  \u001b[38;5;66;03m# --dropout\u001b[39;00m\n\u001b[0;32m    540\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m residual\n\u001b[1;32m--> 541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariant\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maiayn\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariant \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxlm\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariant \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbart\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    542\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[0;32m    544\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TrainModel.main(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7651f4c3",
   "metadata": {},
   "source": [
    "##### Some libraries to bring in requirements\n",
    "ParlaiParser.\n",
    "This is a command line parameter parser for ParlAI. It is specially customized for ParlAI to parse and manage various parameters passed in on the command line. It handles model settings, training configurations and other related parameters.\n",
    "\n",
    "create_agent.\n",
    "This is a utility function to create an agent based on the given settings (usually command line arguments). In ParlAI, an agent can be a model, such as a chatbot, or a person, such as a person interacting with the system via the CLI.\n",
    "\n",
    "DialogPartnerWorld.\n",
    "In ParlAI, a World is an abstract environment in which an agent interacts.DialogPartnerWorld is a specific type of World in which two agents (e.g., a person and a bot) can have a conversation.\n",
    "\n",
    "LocalHumanAgent.\n",
    "This is a special agent that represents a real user that interacts with ParlAI locally (usually a command line interface). It allows a person to interact with other ParlAI agents (e.g. a trained chatbot).\n",
    "\n",
    "WorldLogger.\n",
    "This is a tool class for logging or saving agent interactions in World. This is useful for later analysis or debugging.\n",
    "\n",
    "Message.\n",
    "This is a core class in ParlAI used to represent communication messages between agents. Each message can contain text, labels, rewards and other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a08e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parlai.core.params import ParlaiParser\n",
    "from parlai.core.agents import create_agent\n",
    "from parlai.core.worlds import DialogPartnerWorld\n",
    "from parlai.agents.local_human.local_human import LocalHumanAgent\n",
    "from parlai.utils.world_logging import WorldLogger\n",
    "from parlai.core.message import Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad6c0c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Clear any command line arguments to ensure no Jupyter-related parameters are present\n",
    "sys.argv = ['']\n",
    "\n",
    "params = {\n",
    "    'model_file': 'model/guguAI20',\n",
    "    'dict_file': 'data/models/blender/blender_90M/model.dict',\n",
    "    'dict_tokenizer': 'bpe',\n",
    "    'dict_lower': True,\n",
    "    'gpu': 0,  \n",
    "    'log_keep_fields': 'all',\n",
    "    'outfile': 'log_chartchat_log.txt',\n",
    "    'beam_size': 50,\n",
    "    'inference': 'nucleus',\n",
    "    'top_p': 0.9,\n",
    "    'min_length': 80,\n",
    "    'max_length': 100,\n",
    "    'temperature': 1.2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "404b6ea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:46:42 | \u001b[31mYou gave the print_args flag to parser.parse_args, but this is no longer supported. Use opt.log() to print the arguments\u001b[0m\n",
      "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
      "22:46:42 | \u001b[33mOverriding opt[\"dict_file\"] to data/models/blender/blender_90M/model.dict (previously: model/guguAI20.checkpoint.dict)\u001b[0m\n",
      "22:46:42 | \u001b[33mOverriding opt[\"beam_size\"] to 50 (previously: 1)\u001b[0m\n",
      "22:46:42 | \u001b[33mOverriding opt[\"inference\"] to nucleus (previously: greedy)\u001b[0m\n",
      "22:46:42 | \u001b[33mOverriding opt[\"temperature\"] to 1.2 (previously: 1.0)\u001b[0m\n",
      "22:46:42 | Using CUDA\n",
      "22:46:42 | loading dictionary from model/guguAI20.dict\n",
      "22:46:42 | num words = 54944\n",
      "22:46:43 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "22:46:43 | Loading existing model params from model/guguAI20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'You are guguAI, a female robot.',\n",
       " 'episode_done': False,\n",
       " 'full_text': 'You are guguAI, a female robot.',\n",
       " 'text_vec': tensor([  15,   46,  806,  806, 2395,    6,   12, 1285, 7510,    5]),\n",
       " 'full_text_vec': [15, 46, 806, 806, 2395, 6, 12, 1285, 7510, 5],\n",
       " 'context_original_length': 10,\n",
       " 'context_truncate_rate': False,\n",
       " 'context_truncated_length': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a parser and add command line arguments\n",
    "parser = ParlaiParser(add_model_args=True)\n",
    "parser.set_params(**params)\n",
    "opt = parser.parse_args(print_args=False)\n",
    "\n",
    "# create agents\n",
    "human_agent = LocalHumanAgent(opt)\n",
    "agent = create_agent(opt, requireModelExists=True)\n",
    "\n",
    "# create world\n",
    "world = DialogPartnerWorld(opt, [human_agent, agent])\n",
    "\n",
    "# send initial message to the model\n",
    "initial_message = Message({'text': 'You are guguAI, a female robot.', 'episode_done': False})\n",
    "agent.observe(initial_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfcdc66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your Message: hallo\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mhi there ! i hope you have a nice day or night .\u001b[0;0m\n",
      "Enter Your Message: i have a nice day! what is your name?\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mguguai . i hope you have a good day too .\u001b[0;0m\n",
      "Enter Your Message: thank you ,i am doing my homework\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mthat ' s fine . i am doing my homework too .\u001b[0;0m\n",
      "Enter Your Message: what type of home work have you did?\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi do sales work . what about you .\u001b[0;0m\n",
      "Enter Your Message: my homework about AI for meida\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mhow long have you been working on it ?\u001b[0;0m\n",
      "Enter Your Message: about 5weeks\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mwow . working on it yourself ?\u001b[0;0m\n",
      "Enter Your Message: yes, ALL myself\n",
      "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mwow . must be really busy .\u001b[0;0m\n",
      "Chat ended with KeyboardInterrupt.\n"
     ]
    }
   ],
   "source": [
    "# create logger\n",
    "logger = WorldLogger(opt)\n",
    "\n",
    "# chat\n",
    "try:\n",
    "    while True:\n",
    "        # let human agent act first\n",
    "        if agent.observation is None:\n",
    "            world.parley()\n",
    "        world.parley()\n",
    "        logger.log(world)\n",
    "        if world.epoch_done():\n",
    "            print(\"EPOCH DONE\")\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    print('Chat ended with KeyboardInterrupt.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "291fe751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:25:06 | Saving log to log_chartchat_log.txt in ParlAI format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# save chat logs\n",
    "logger.reset_world()  # this is needed to flush the last few messages\n",
    "logger.write_parlai_format(opt['outfile'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778eba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
